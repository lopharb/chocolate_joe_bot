from ollama import chat


class OllamaModel:
    """
    Handles interactions with an Ollama model.

    This class provides an interface for communicating with Ollama's large language models.
    It allows for sending messages to the model and processing the responses, with options
    to control model loading behavior and response formatting.
    """

    def __init__(self, model_name: str, unload: int = 300) -> None:
        """
        Initializes the OllamaModel class with the specified model name.

        Args:
            model_name (str): Name of the model to be used.
            unload (int, optional): Number of seconds to keep the model loaded after the last response.
                Defaults to 300. Negative numbers keep the model loaded permanently.
        """
        self.model_name = model_name
        self.unload_timer = unload

    def get_response(
        self, messages: list[dict[str, str]], include_reasoning: bool = False
    ) -> str:
        """
        Generates a response from an Ollama model based on input messages.

        Args:
            messages (list[dict[str, str]]): List of messages, where each message is a dictionary
                containing 'role' and 'content' keys.
            include_reasoning (bool, optional): Whether to include reasoning in the response. When True,
                the model's full reasoning process is included. When False, only the final answer is returned.
                Defaults to False.

        Returns:
            str: The processed response generated by the model.
        """
        response = chat(
            model=self.model_name,
            messages=messages,
            stream=False,
            keep_alive=self.unload_timer,  # keeps the model loaded for the specified time
            options={
                "temperature": 1.0  # higher temperature for more creative responses
            },
        )

        result = response.message.content or ""

        return result
